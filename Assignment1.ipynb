{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment1.ipynb","provenance":[{"file_id":"1Vld5m0Y-gzfAbciT9UqT1-X7Xbw3mHID","timestamp":1583677028459},{"file_id":"https://github.com/janchorowski/dl_uwr/blob/summer2020/Assignment1/Assignment1.ipynb","timestamp":1583007063032}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zAiECkYEaZn1","colab_type":"text"},"source":["# Assignment 1\n","\n","## Important notes\n","**Submission deadline:**\n","* **Thursday, 12.03.2020**\n","\n","**Points: 13 + 2bp**"]},{"cell_type":"markdown","metadata":{"id":"Bb0zN1GvapSt","colab_type":"text"},"source":["This assignment is meant to test your skills in course pre-requisites:  Scientific Python programming and  Machine Learning. If it is hard, I strongly advise you to drop the course.\n","\n","Please use GitHub’s [pull requests](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests) and issues to send corrections!\n","\n","You can solve the assignment in any system you like, but we encourage you to try out [Google Colab](https://colab.research.google.com/)."]},{"cell_type":"markdown","metadata":{"id":"zIBf-IF_ahTI","colab_type":"text"},"source":["## Assignment text\n","1. **[1p]** Download data competition from a Kaggle competition on sentiment prediction from [[https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data)].  Keep only full sentences, i.e. for each `SenteceId` keep only the entry with the lowest `PhraseId`.  Use first 7000 sentences as a `train set` and the remaining 1529 sentences as the `test set`. \n","\n","2. **[1p]** Prepare the data for logistic regression:\n","\tMap the sentiment scores $0,1,2,3,4$ to a probability of the sentence being by setting $p(\\textrm{positive}) = \\textrm{sentiment}/4$.\n","\tBuild a dictionary of most frequent 20000 words.\n","\n","3. **[3p]** Treat each document as a bag of words. e.g. if the vocabulary is \n","\t```\n","\t0: the\n","\t1: good\n","\t2: movie\n","\t3: is\n","\t4: not\n","\t5: a\n","\t6: funny\n","\t```\n","\tThen the encodings can be:\n","\t```\n","\tgood:                           [0,1,0,0,0,0,0]\n","\tnot good:                       [0,1,0,0,1,0,0] \n","\tthe movie is not a funny movie: [1,0,2,1,1,1,1]\n","\t```\n","    Train a logistic regression model to predict the sentiment. Compute the correlation between the predicted probabilities and the sentiment. Record the most positive and negative words.\n","    Please note that in this model each word gets its sentiment parameter $S_w$ and the score for a sentence is \n","    $$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}S_w$$\n","\n","4. **[3p]** Now prepare an encoding in which negation flips the sign of the following words. For instance for our vocabulary the encodings become:\n","\t```\n","\tgood:                           [0,1,0,0,0,0,0]\n","\tnot good:                       [0,-1,0,0,1,0,0]\n","\tnot not good:                   [0,1,0,0,0,0,0]\n","\tthe movie is not a funny movie: [1,0,0,1,1,-1,-1]\n","\t```\n","\tFor best results, you will probably need to construct a list of negative words.\n","\t\n","\tAgain train a logistic regression classifier and compare the results to the Bag of Words approach.\n","\t\n","\tPlease note that this model still maintains a single parameter for each word, but now the sentence score is\n","\t$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}-1^{\\text{count of negations preceeding }w}S_w$$\n","\n","5. **[5p]** Now also consider emphasizing words such as `very`. They can boost (multiply by a constant >1) the following words.\n","\tImplement learning the modifying multiplier for negation and for emphasis. One way to do this is to introduce a model which has:\n","\t- two modifiers, $N$ for negation and $E$ for emphasis\n","\t- a sentiment score $S_w$ for each word \n","And score each sentence as:\n","$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}N^{\\text{\\#negs prec. }w}E^{\\text{\\#emphs prec. }w}S_w$$\n","\n","You will need to implement a custom logistic regression model to support it.\n","\n","6. **[2pb]** Propose, implement, and evaluate an extension to the above model.\n"]},{"cell_type":"code","metadata":{"id":"G66ys_nYCl49","colab_type":"code","colab":{}},"source":["import csv\n","import numpy as np\n","import scipy.optimize as sopt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CsIwzxB2AUkm","colab_type":"code","outputId":"b6bd0c3e-bdc0-4190-efeb-59109056fe08","executionInfo":{"status":"ok","timestamp":1583665730287,"user_tz":-60,"elapsed":30692,"user":{"displayName":"Mateusz Basiak","photoUrl":"","userId":"03396677680592115446"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N_kvBtjWm9VC","colab_type":"code","outputId":"07b24f94-6c16-43eb-86cf-6c2e907efb74","executionInfo":{"status":"ok","timestamp":1583665735853,"user_tz":-60,"elapsed":3888,"user":{"displayName":"Mateusz Basiak","photoUrl":"","userId":"03396677680592115446"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["class new_sentence():\n","  def __init__(self, row):\n","    self.id    = row[1]\n","    self.words = row[2].split()\n","    self.prob  = float(row[3]) / 4.0\n","\n","with open('drive/My Drive/Uczelnia/Neurony/train.tsv') as tsvfile:\n","  reader = csv.reader(tsvfile, delimiter='\\t')\n","  whole_set = []\n","  words     = []\n","  for row in reader:\n","    if row[0] != 'PhraseId':\n","      sen = new_sentence(row)\n","      if (len(whole_set) == 0 or sen.id != whole_set[-1].id) and len(sen.words) > 0:\n","        whole_set.append(sen)\n","        for word in sen.words:\n","          words.append(word)\n","\n","train_set = whole_set[0:7000]\n","test_set  = whole_set[7000:]\n","print(\"Size of training set:\", len(train_set))\n","print(\"Size of test set:\", len(test_set))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Size of training set: 7000\n","Size of test set: 1529\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2aqYGvp3IdRP","colab_type":"code","outputId":"d70bcf36-e11e-4729-985f-e2d2282a5bf6","executionInfo":{"status":"ok","timestamp":1583665742302,"user_tz":-60,"elapsed":2258,"user":{"displayName":"Mateusz Basiak","photoUrl":"","userId":"03396677680592115446"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["unique_words = list(set(words))\n","# count_words = []\n","# for word in unique_words:\n","#   num = words.count(word)\n","#   count_words.append([num, word])\n","\n","# count_words.sort(reverse = True)\n","# vocab = count_words[:2000]\n","# voc = []\n","# for p in vocab:\n","#   voc.append(p[1])\n","\n","voc = unique_words\n","rev_voc = {}\n","for i, word in enumerate(voc):\n","  rev_voc[word] = i\n","print(\"Size of vocabulary:\", len(voc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Size of vocabulary: 18132\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qXESEcovPstV","colab_type":"code","outputId":"b2809e40-9766-43d3-845b-e15908114c5d","executionInfo":{"status":"ok","timestamp":1583666976839,"user_tz":-60,"elapsed":1224372,"user":{"displayName":"Mateusz Basiak","photoUrl":"","userId":"03396677680592115446"}},"colab":{"base_uri":"https://localhost:8080/","height":665}},"source":["# Task 3\n","def logis(encoding, sentiment):\n","  theta = np.exp(np.int(-1.0) * np.dot(encoding, sentiment))\n","  return np.int(1.0) / (np.int(1.0) + theta)\n","\n","def log_loss(encoding, sentiment, true_sentiment):\n","  h = logis(encoding, sentiment)\n","  err = np.double(0.0)\n","  if h != np.double(0.0):\n","    err -= true_sentiment * np.log(h)\n","  if h != np.double(1.0):\n","    err -= (np.double(1.0) - true_sentiment) * np.log(np.double(1.0) - h)\n","  grad = (h - true_sentiment) * encoding\n","  return err, grad\n","\n","def upd_sent(sentiment):\n","  g_loss = g_grad = np.double(0.0)\n","  for sentence in train_set:\n","    enc = np.zeros(len(voc) + 1)\n","    enc[-1] = np.double(1.0)\n","    for word in sentence.words:\n","      if word in voc:\n","        enc[rev_voc[word]] += np.double(1.0)\n","    loss, grad = log_loss(enc, sentiment, sentence.prob)\n","    g_loss += loss\n","    g_grad += grad\n","  print(g_loss)\n","  return g_loss, g_grad\n","\n","word_sent = sopt.fmin_l_bfgs_b(lambda x: upd_sent(x), np.zeros(len(voc) + 1),\n","                               maxiter = 30)[0]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["4852.030263920194\n","6682.810661488786\n","4832.42270415843\n","4818.071644032589\n","4773.902116862635\n","4683.3468953965\n","4594.711881327117\n","4497.299781346748\n","4407.827214460151\n","4326.51255404781\n","4236.234257282805\n","4175.5215826398235\n","4113.619751579541\n","4030.1301766571028\n","4708.862187668917\n","4020.3971759665806\n","3971.5194076295334\n","3948.8750465177463\n","3922.9282429251693\n","3897.9841121548516\n","3865.359790532102\n","3818.2200511894334\n","3850.9357327254857\n","3794.366479807621\n","3750.617594244316\n","3721.894481038437\n","3697.9030127471533\n","3682.3198604092104\n","3672.5446576580234\n","3663.7287940007786\n","3655.9120827521733\n","3645.05298447347\n","3623.8626094171454\n","3658.706109169818\n","3608.1224928068036\n","3587.6271909239363\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8a_6ZnGGiC0x","colab_type":"code","outputId":"15a08756-dc7d-4903-8eae-5c0f8b6bc960","executionInfo":{"status":"ok","timestamp":1583667594138,"user_tz":-60,"elapsed":9608,"user":{"displayName":"Mateusz Basiak","photoUrl":"","userId":"03396677680592115446"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["error = 0.0\n","mse   = 0.0\n","for sentence in test_set:\n","  enc = np.zeros(len(voc) + 1)\n","  for word in sentence.words:\n","    if word in voc:\n","      enc[rev_voc[word]] += np.double(1.0)\n","  error += log_loss(enc, word_sent, sentence.prob)[0]\n","  mse   += (sentence.prob - logis(enc, word_sent)) ** 2\n","\n","print(\"Final error:\", error)\n","print(\"Mean square error:\", mse)\n","print(\"Most negative word:\", \n","      voc[np.where(word_sent == np.amin(word_sent))[0][0]])\n","print(\"Most positive word:\", \n","      voc[np.where(word_sent == np.amax(word_sent))[0][0]])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Final error: 985.1239688319794\n","Mean square error: 112.15649172292257\n","Most negative word: worst\n","Most positive word: remarkable\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WBAF6kKBcbyM","colab_type":"code","outputId":"71dbb3ba-0555-4547-8b8c-62a35f6e539f","executionInfo":{"status":"ok","timestamp":1583668721030,"user_tz":-60,"elapsed":1118901,"user":{"displayName":"Mateusz Basiak","photoUrl":"","userId":"03396677680592115446"}},"colab":{"base_uri":"https://localhost:8080/","height":611}},"source":["# Task 4\n","neg_words = ['not', 'no', 'neither', 'never', 'no one', 'nobody',\n","             'none', 'nor', 'nothing', 'nowhere'] # from Cambridge Dictionary\n","\n","def upd_sent_neg(sentiment):\n","  g_loss = g_grad = np.double(0.0)\n","  for sentence in train_set:\n","    enc = np.zeros(len(voc) + 1)\n","    word_mean = np.double(1.0)\n","    enc[-1] = np.double(1.0)\n","    for word in sentence.words:\n","      if word in voc:\n","        enc[rev_voc[word]] += word_mean\n","      if word in neg_words:\n","        word_mean *= np.double(-1.0)\n","    loss, grad = log_loss(enc, sentiment, sentence.prob)\n","    g_loss += loss\n","    g_grad += grad\n","  print(g_loss)\n","  return g_loss, g_grad\n","\n","word_sent_neg = sopt.fmin_l_bfgs_b(lambda x: upd_sent_neg(x),\n","                                   np.zeros(len(voc) + 1), maxiter = 30)[0]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["4852.030263920194\n","6623.704555001974\n","4830.81464143412\n","4817.439739341191\n","4775.581803605916\n","4688.073818127904\n","4612.006533919176\n","4526.571866383059\n","4442.84377882224\n","4347.227986459409\n","4292.927824632196\n","4235.983020532392\n","4195.428572665868\n","4155.692802962758\n","4085.7699836527618\n","4065.7853795727115\n","4007.3837719229646\n","3994.892680414913\n","3984.0242641688374\n","3946.120165980295\n","3882.7765868056968\n","3868.5339147146547\n","3822.7383772814164\n","3806.287551870615\n","3779.223938750483\n","3749.693442828891\n","3724.8472006308734\n","3710.8595998510013\n","3703.8588494037267\n","3693.0693849272743\n","3678.841559515922\n","3657.347072309784\n","3622.508653495511\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gp3VhUtUDnas","colab_type":"code","outputId":"68eb855e-a388-4fd0-d78e-59d5034b367e","executionInfo":{"status":"ok","timestamp":1583673264147,"user_tz":-60,"elapsed":9077,"user":{"displayName":"Mateusz Basiak","photoUrl":"","userId":"03396677680592115446"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["err_with_neg = 0.0\n","mse_with_neg = 0.0\n","for sentence in test_set:\n","  enc = np.zeros(len(voc) + 1)\n","  word_mean = np.double(1.0)\n","  enc[-1] = np.double(1.0)\n","  for word in sentence.words:\n","    if word in voc:\n","      enc[rev_voc[word]] += word_mean\n","    if word in neg_words:\n","      word_mean *= np.double(-1.0)\n","  err_with_neg += log_loss(enc, word_sent_neg, sentence.prob)[0]\n","  mse_with_neg += (sentence.prob - logis(enc, word_sent_neg)) ** 2\n","\n","print(\"Final error:\", err_with_neg)\n","print(\"Mean square error:\", mse_with_neg)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Final error: 1003.0639712672682\n","Mean square error: 120.91617474109681\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3ludJ2mb-cBo","colab_type":"code","outputId":"058d7970-27fa-4157-82c3-58ff2d9b8f18","executionInfo":{"status":"ok","timestamp":1583674890615,"user_tz":-60,"elapsed":1605660,"user":{"displayName":"Mateusz Basiak","photoUrl":"","userId":"03396677680592115446"}},"colab":{"base_uri":"https://localhost:8080/","height":701}},"source":["# Task 5\n","emph_words = ['very', 'really', 'much', 'more', 'extremely']\n","length = len(voc) + 3     # bias, negation multiplier N, emphasis multiplier E\n","word_sent_def = np.zeros(length)\n","word_sent_def[-2] = np.double(-1.0) \n","word_sent_def[-1] = np.double(1.5)\n","\n","def log_loss_param(encoding, sentiment, true_sentiment, deN, deE):\n","  h = logis(encoding, sentiment)\n","  err = np.double(0.0)\n","  if h != np.double(0.0):\n","    err -= true_sentiment * np.log(h)\n","  if h != np.double(1.0):\n","    err -= (np.double(1.0) - true_sentiment) * np.log(np.double(1.0) - h)\n","  grad = (h - true_sentiment) * encoding\n","  grad[-2] = np.sum((h - true_sentiment) * np.dot(deN, sentiment))\n","  grad[-1] = np.sum((h - true_sentiment) * np.dot(deE, sentiment))\n","  return err, grad\n","\n","def upd_sent_param(sentiment):\n","  g_loss = g_grad = np.double(0.0)\n","  for sentence in train_set:\n","    enc = np.zeros(length)\n","    enc[-3] = np.double(1.0)\n","    N = sentiment[-2]\n","    E = sentiment[-1]\n","    num_neg = num_emp = np.double(0.0)\n","    dN = dE = np.zeros(length)\n","\n","    for word in sentence.words:\n","      if word in voc:\n","        enc[rev_voc[word]] += np.prod(np.power([N, E], [num_neg, num_emp]))\n","        dN[rev_voc[word]]  += num_neg * np.prod(np.power([N, E], [num_neg - 1, num_emp]))\n","        dE[rev_voc[word]]  += num_emp * np.prod(np.power([N, E], [num_neg, num_emp - 1]))\n","      if word in neg_words:\n","        num_neg += 1\n","      if word in emph_words:\n","        num_emp += 1\n","    loss, grad = log_loss_param(enc, sentiment, sentence.prob, dN, dE)\n","    g_loss += loss\n","    g_grad += grad\n","  print(g_loss)\n","  return g_loss, g_grad\n","\n","\n","word_sent_param = sopt.fmin_l_bfgs_b(lambda x: upd_sent_param(x),\n","                                   word_sent_def, maxiter = 30)[0]\n","\n","print(\"Negation parameter:\", word_sent_param[-2])\n","print(\"Emphasis parameter:\", word_sent_param[-1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["4852.030263920194\n","6742.334748120995\n","4832.372235535328\n","4819.602303515584\n","4779.344591520989\n","4689.970614881983\n","4612.211899534437\n","4527.447412993914\n","4445.942617123351\n","4340.733697080622\n","4306.609613316562\n","4244.193660429922\n","4209.953877424586\n","4172.610532168653\n","4107.047550604742\n","4127.038495548291\n","4067.0915793820163\n","4031.6219256147447\n","4012.696044121923\n","3994.273420364525\n","3932.6607531045506\n","3931.811546993564\n","3903.075998070227\n","3877.6116028777583\n","3841.84696604829\n","3808.2659625238302\n","3785.3324351671254\n","3753.0045971694094\n","3746.510588674248\n","3739.995547404545\n","3733.7673484996126\n","3725.266157999171\n","3708.449599381723\n","3678.317795405719\n","3717.6641419835482\n","3662.1089207997607\n","Negation parameter: -0.4198439752434978\n","Emphasis parameter: 2.0801560247564934\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OrtAEmUfGy75","colab_type":"code","outputId":"c6e6a7c2-3d24-487d-d7b4-9512c920136d","executionInfo":{"status":"ok","timestamp":1583677013909,"user_tz":-60,"elapsed":10342,"user":{"displayName":"Mateusz Basiak","photoUrl":"","userId":"03396677680592115446"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["err_param = 0.0\n","mse_param = 0.0\n","for sentence in test_set:\n","  enc = np.zeros(length)\n","  enc[-3] = np.double(1.0)\n","  N = word_sent_param[-2]\n","  E = word_sent_param[-1]\n","  num_neg = num_emp = np.double(0.0)\n","\n","  for word in sentence.words:\n","    if word in voc:\n","      enc[rev_voc[word]] += np.prod(np.power([N, E], [num_neg, num_emp]))\n","    if word in neg_words:\n","      num_neg += 1\n","    if word in emph_words:\n","      num_emp += 1\n","  err_param += log_loss_param(enc, word_sent_param, sentence.prob, 0, 0)[0]\n","  mse_param += (sentence.prob - logis(enc, word_sent_param)) ** 2\n","\n","print(\"Final error:\", err_param)\n","print(\"Mean square error:\", mse_param)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Final error: 997.2152547064817\n","Mean square error: 117.03937884708144\n"],"name":"stdout"}]}]}