{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment2_Basiak_Mateusz.ipynb","provenance":[{"file_id":"https://github.com/janchorowski/dl_uwr/blob/summer2020/Assignment2/Assignment2.ipynb","timestamp":1584207889822}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CGXgWugfJ0Vl","colab_type":"text"},"source":["# Assignment 2\n","\n","**Submission deadline: last lab session before or on Thursday, 26.03.2020**\n","\n","**Points: 6 + 1 bonus points**\n","\n","## Submission instructions\n","The class is held remotely. To sumbmit your solutions please save the notebook to your Google Drive, then:\n","1. Rename it it to: Assignment2_Surname_FirstName\n","2. Rerun the whole notebook `Runtime -> Restar and run all`\n","3. Make a pinned revision `File->Save and pin revision`\n","4. Share the notebook with your instructor using his `cs.uni.wroc.pl` email\n","\n","We will use the commenting system and video conferences to check and discuss the solutions.\n","\n","As always, please submit corrections using GitHub's Pull Requests."]},{"cell_type":"code","metadata":{"id":"nfVDe-bMqVT_","colab_type":"code","colab":{}},"source":["%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YiTEWD2oqW0Y","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RZCM_hdELE04","colab_type":"text"},"source":["The code below contains a mock-up of a two-layer neural network. Fill in the code and manually set weights to solve the XOR problem.\n","\n","Please note: the shapes are set to be compatible with PyTorch's conventions:\n","* a batch containing $N$ $D$-dimensional examples has shape $N\\times D$ (each example is a row!)\n","* a weight matrix in a linear layer with $I$ inputs and $O$ outputs has shape $O \\times I$\n","* a bias vector is a 1D vector. Please note that [broadcasting rules](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) allow us to think about it as a $1 \\times D` matrix."]},{"cell_type":"code","metadata":{"id":"lYEbCfbSpv5M","colab_type":"code","outputId":"09129ae9-a3dd-463a-fcb1-750b16f4c880","executionInfo":{"status":"ok","timestamp":1584207972337,"user_tz":-60,"elapsed":1399,"user":{"displayName":"Mateusz Basiak","photoUrl":"","userId":"03396677680592115446"}},"colab":{"base_uri":"https://localhost:8080/","height":297}},"source":["# Let's define a XOR dataset\n","\n","# X will be matrix of N 2-dimensional inputs\n","X = np.array(\n","    [[0, 0],\n","     [0, 1],\n","     [1, 0],\n","     [1, 1],\n","    ], dtype=np.float32)\n","# Y is a matrix of N numbers - answers\n","Y = np.array(\n","    [[0],\n","     [1],\n","     [1],\n","     [0],\n","    ], dtype=np.float32)\n","\n","plt.scatter(X[:,0], X[:,1], c=Y[:,0], )\n","plt.xlabel('X[0]')\n","plt.ylabel('X[1]')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'X[1]')"]},"metadata":{"tags":[]},"execution_count":3},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Rb3azMn929_I","colab_type":"text"},"source":["# Problem 1 [2p]\n","\n","Fill in the details of a forward pass, then manually set the weights and biases in the network to solve the 2D XOR task defined above."]},{"cell_type":"code","metadata":{"id":"lrrRuk6zLiF0","colab_type":"code","colab":{}},"source":["def sigmoid(x):\n","    return np.double(1.0) / (np.double(1.0) + np.exp(-x))\n","\n","class SmallNet:\n","    def __init__(self, in_features, num_hidden, dtype=np.float32):\n","        self.W1 = np.zeros((num_hidden, in_features), dtype=dtype)\n","        self.b1 = np.zeros((num_hidden,), dtype=dtype)\n","        self.W2 = np.zeros((1, num_hidden), dtype=dtype)\n","        self.b2 = np.zeros((1,), dtype=dtype)\n","        self.init_params()\n","\n","    def init_params(self):\n","        # Problem 2:\n","        # set all parameters to small random values, e.g. from N(0, 0.5)\n","        self.W1 = np.random.normal(0, 0.5, self.W1.shape)\n","        self.b1 = np.random.normal(0, 0.5, self.b1.shape)\n","        self.W2 = np.random.normal(0, 0.5, self.W2.shape)\n","        self.b2 = np.random.normal(0, 0.5, self.b2.shape)\n","\n","    def forward(self, X, Y=None, do_backward=False):\n","        # Problem 1: Fill in details of forward propagation\n","\n","        # Input to neurons in 1st layer\n","        A1 = np.matmul(X, self.W1.transpose()) + self.b1\n","        # Outputs after the sigmoid non-linearity\n","        O1 = sigmoid(A1)\n","        # Inputs to neuron in the second layer\n","        A2 = np.matmul(O1, self.W2.transpose()) + self.b2\n","        # Outputs after the sigmoid non-linearity\n","        O2 = sigmoid(A2)\n","\n","        if Y is not None:\n","            loss = - Y * np.log(O2) - (np.double(1.0) - Y) * np.log(np.double(1.0) - O2)\n","            # normalize loss by batch size\n","            loss = loss.sum() / X.shape[0]\n","        else:\n","            loss = np.nan\n","\n","        if do_backward:\n","            # Problem 2: fill in the gradient computation\n","            # Please note, thate there is a correspondance between\n","            # the forward and backward pass: with backward computations happening\n","            # in reversed order. \n","\n","            # A2_grad is the gradient of loss with respect to A2\n","            # Hint: there is a concise formula for the gradient \n","            # of logistic sigmoid and cross-entropy loss\n","            batch_size = X.shape[0] \n","            A2_grad = O2 - Y\n","            self.b2_grad = A2_grad.sum(0) / batch_size\n","            self.W2_grad = np.multiply(O1, \n","                                       np.repeat(A2_grad, O1.shape[1], axis=1)\n","                                       ).sum(0)[np.newaxis, :] / batch_size\n","            O1_grad = np.dot(A2_grad, self.W2)\n","            A1_grad = np.multiply(O1_grad, np.multiply(O1, (1 - O1)))\n","            self.b1_grad = A1_grad.sum(0) / batch_size\n","            self.W1_grad = np.multiply(np.repeat(X[:, np.newaxis, :],\n","                                                 A1_grad.shape[1],\n","                                                 axis = 1),\n","                                       np.repeat(A1_grad[:, :, np.newaxis],\n","                                                 X.shape[1], \n","                                                 axis = 2)).sum(0) / batch_size\n","\n","        return O2, loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jJswvBk0oiIY","colab_type":"code","outputId":"45e2245a-0139-47f9-fc49-988c11781d28","executionInfo":{"status":"ok","timestamp":1584207972342,"user_tz":-60,"elapsed":1387,"user":{"displayName":"Mateusz Basiak","photoUrl":"","userId":"03396677680592115446"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# Problem 1:\n","# Set by hand the weight values to solve the XOR problem\n","\n","net = SmallNet(2, 2, dtype=np.float64)\n","net.W1 = np.array([[5.0, 5.0], [5.0, 5.0]])\n","net.b1 = np.array([-2.5, -7.5])\n","net.W2 = np.array([[5.0, -5.0]])\n","net.b2 = np.array([-2.5])\n","\n","# Hint: since we use the logistic sigmoid activation, the weights may need to \n","# be fairly large \n","\n","\n","net.forward(X, Y, do_backward=True)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[0.10683622],\n","        [0.85086711],\n","        [0.85086711],\n","        [0.10683622]]), 0.13724231150961305)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"wmxCi5Vl6_xB","colab_type":"text"},"source":["# Problem 2 [2p]\n","\n","1. Fill in the backward pass.\n","2. Implement random initialization of network parameters.\n","3. Using `float64` verify correctness of your backward pass."]},{"cell_type":"code","metadata":{"id":"eSM5hgJ1mrhY","colab_type":"code","colab":{}},"source":["def check_grad(net, param_name, X, Y, eps=1e-5):\n","    \"\"\"A gradient checking routine\"\"\"\n","    \n","    param = getattr(net, param_name)\n","    param_flat_accessor = param.reshape(-1)\n","\n","    grad = np.empty_like(param)\n","    grad_flat_accessor = grad.reshape(-1)\n","\n","    net.forward(X, Y, do_backward=True)\n","    orig_grad = getattr(net, param_name + '_grad')\n","    assert (param.shape == orig_grad.shape)\n","\n","    for i in range(param_flat_accessor.shape[0]):\n","        orig_val = param_flat_accessor[i]\n","        param_flat_accessor[i] = orig_val + eps\n","        _, loss_positive = net.forward(X, Y)\n","        param_flat_accessor[i] = orig_val - eps\n","        _, loss_negative = net.forward(X, Y)\n","        param_flat_accessor[i] = orig_val\n","        grad_flat_accessor[i] = (\n","            loss_positive - loss_negative) / (2 * eps)\n","    assert np.allclose(grad, orig_grad)\n","    return grad, orig_grad"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTZu0jFEvgXF","colab_type":"code","colab":{}},"source":["net = SmallNet(2, 2, dtype=np.float64)\n","\n","for param_name in ['W1', 'b1', 'W2', 'b2']:\n","    check_grad(net, param_name, X, Y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8mUOs3cVvjM2","colab_type":"text"},"source":["# Problem 3 [2p]\n","\n","Fill in the details of batch gradient descent below, then train a network to solve 2D XOR problem.\n","\n","Then test the reliability of solving the 3D XOR task."]},{"cell_type":"code","metadata":{"id":"nn2AAoZo0vjU","colab_type":"code","outputId":"edfb8e7b-9bfe-41e0-9e59-1643be583cae","executionInfo":{"status":"ok","timestamp":1584207984253,"user_tz":-60,"elapsed":13281,"user":{"displayName":"Mateusz Basiak","photoUrl":"","userId":"03396677680592115446"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["net = SmallNet(2, 10, dtype=np.float64)\n","\n","alpha = 1e-1\n","\n","for i in range(100000):\n","    _, loss = net.forward(X, Y, do_backward=True)\n","    if (i % 5000) == 0:\n","        print(i, loss)\n","    for param_name in ['W1', 'b1', 'W2', 'b2']:\n","        param = getattr(net, param_name)\n","        # Hint: use the construct `param[:]` to change the contents of the array!\n","        # Doing instead `param = new_val` simply changes to what the variable\n","        # param points to, without affecting the network!\n","        grad = getattr(net, param_name + '_grad')\n","        param[:] = param - grad"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 0.7017737378746061\n","5000 0.0009439128818267353\n","10000 0.0004259388180536731\n","15000 0.00027158447687281886\n","20000 0.0001982946985474811\n","25000 0.00015570827053192513\n","30000 0.00012795229973020845\n","35000 0.00010846422201950202\n","40000 9.404676977389029e-05\n","45000 8.29588793074614e-05\n","50000 7.417262590415466e-05\n","55000 6.704258505395904e-05\n","60000 6.114335044638583e-05\n","65000 5.6183301929908986e-05\n","70000 5.195593671920855e-05\n","75000 4.831098105027684e-05\n","80000 4.513650565027148e-05\n","85000 4.234745096686793e-05\n","90000 3.987803538204718e-05\n","95000 3.767659659830131e-05\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TwpEjpkU1JvK","colab_type":"code","outputId":"33ce422d-7de2-4b1e-8274-84d4eabced8b","executionInfo":{"status":"ok","timestamp":1584207984254,"user_tz":-60,"elapsed":13273,"user":{"displayName":"Mateusz Basiak","photoUrl":"","userId":"03396677680592115446"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["net.forward(X, Y, do_backward=True)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[2.65360389e-05],\n","        [9.99974527e-01],\n","        [9.99954949e-01],\n","        [4.57449315e-05]]), 3.570200331866367e-05)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"w3NiL0ku2Unr","colab_type":"text"},"source":["Generate below data for a 3D XOR task. Try a few values of hidden layer size. Plot the reliability of training, i.e. how many trainings succeed for a given hiden layer size.\n","\n","What is easier to train: a smaller, or large network?"]},{"cell_type":"code","metadata":{"id":"U0ZMyHqz8xrC","colab_type":"code","outputId":"366d3edd-2cce-42eb-b977-583bd54069e1","executionInfo":{"status":"ok","timestamp":1584208283183,"user_tz":-60,"elapsed":312193,"user":{"displayName":"Mateusz Basiak","photoUrl":"","userId":"03396677680592115446"}},"colab":{"base_uri":"https://localhost:8080/","height":287}},"source":["X3 = np.array(\n","    [[0, 0, 0],\n","     [0, 0, 1],\n","     [0, 1, 0],\n","     [0, 1, 1],\n","     [1, 0, 0],\n","     [1, 0, 1],\n","     [1, 1, 0],\n","     [1, 1, 1],\n","    ], dtype=np.float32)\n","Y3 = np.array(\n","    [[0],\n","     [1],\n","     [1],\n","     [0],\n","     [1],\n","     [0],\n","     [0],\n","     [0],\n","    ], dtype=np.float32)\n","\n","alpha = 0.1\n","for hidden_dim in [2, 3, 5, 10, 20]:\n","    # Run a few trainings and record the fraction of successful ones\n","\n","    succ_trainings = 0.0\n","    for train in range(50):\n","        net = SmallNet(3, hidden_dim, dtype=np.float64)\n","        for i in range(10000):\n","            _, loss = net.forward(X3, Y3, do_backward=True)\n","            for param_name in ['W1', 'b1', 'W2', 'b2']:\n","                param = getattr(net, param_name)\n","                grad  = getattr(net, param_name + '_grad')\n","                param[:] = param - grad\n","            if i == 9999 and loss < 0.001:\n","                succ_trainings += 0.02\n","    print(\"Hidden layer size:\", hidden_dim)\n","    print(\"Succesfull trainings:\", np.around(succ_trainings, decimals=2), \"\\n\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Hidden layer size: 2\n","Succesfull trainings: 0.0 \n","\n","Hidden layer size: 3\n","Succesfull trainings: 0.64 \n","\n","Hidden layer size: 5\n","Succesfull trainings: 1.0 \n","\n","Hidden layer size: 10\n","Succesfull trainings: 1.0 \n","\n","Hidden layer size: 20\n","Succesfull trainings: 1.0 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UuaLEoV-9DLG","colab_type":"text"},"source":["# Problem 4 [1bp]\n","\n","Replace the first nonlinearity with the [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation function. Verify ho"]},{"cell_type":"code","metadata":{"id":"lZ4fDG3__lpF","colab_type":"code","colab":{}},"source":["def ReLU(x):\n","    return np.maximum(x, 0, x)\n","\n","class ReLUNet:\n","    def __init__(self, in_features, num_hidden, dtype=np.float32):\n","        self.W1 = np.zeros((num_hidden, in_features), dtype=dtype)\n","        self.b1 = np.zeros((num_hidden,), dtype=dtype)\n","        self.W2 = np.zeros((1, num_hidden), dtype=dtype)\n","        self.b2 = np.zeros((1,), dtype=dtype)\n","        self.init_params()\n","\n","    def init_params(self):\n","        self.W1 = np.random.normal(0, 0.5, self.W1.shape)\n","        self.b1 = np.random.normal(0, 0.5, self.b1.shape)\n","        self.W2 = np.random.normal(0, 0.5, self.W2.shape)\n","        self.b2 = np.random.normal(0, 0.5, self.b2.shape)\n","\n","    def forward(self, X, Y=None, do_backward=False):\n","        A1 = np.matmul(X, self.W1.transpose()) + self.b1\n","        O1 = ReLU(A1)\n","        A2 = np.matmul(O1, self.W2.transpose()) + self.b2\n","        O2 = sigmoid(A2)\n","\n","        if Y is not None:\n","            loss = - Y * np.log(O2) - (np.double(1.0) - Y) * np.log(np.double(1.0) - O2)\n","            loss = loss.sum() / X.shape[0]\n","        else:\n","            loss = np.nan\n","\n","        if do_backward:\n","            batch_size = X.shape[0] \n","            A2_grad = O2 - Y\n","            self.b2_grad = A2_grad.sum(0) / batch_size\n","            self.W2_grad = np.multiply(O1, \n","                                       np.repeat(A2_grad, O1.shape[1], axis=1)\n","                                       ).sum(0)[np.newaxis, :] / batch_size\n","            O1_grad = np.dot(A2_grad, self.W2)\n","            A1_grad = np.multiply(O1_grad, np.multiply(O1, (1 - O1)))\n","            self.b1_grad = A1_grad.sum(0) / batch_size\n","            self.W1_grad = np.multiply(np.repeat(X[:, np.newaxis, :],\n","                                                 A1_grad.shape[1],\n","                                                 axis = 1),\n","                                       np.repeat(A1_grad[:, :, np.newaxis],\n","                                                 X.shape[1], \n","                                                 axis = 2)).sum(0) / batch_size\n","\n","        return O2, loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QBfmb7EIAf4u","colab_type":"code","outputId":"3c88679d-f8b4-45ac-8a0c-73f23f23c533","executionInfo":{"status":"ok","timestamp":1584208283185,"user_tz":-60,"elapsed":312181,"user":{"displayName":"Mateusz Basiak","photoUrl":"","userId":"03396677680592115446"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["net = ReLUNet(3, 10, dtype=np.float64)\n","\n","alpha = 1e-1\n","\n","for i in range(500):          # weights of matrixes get really big really fast, NaNs start to appear\n","    _, loss = net.forward(X3, Y3, do_backward=True)\n","    if (i % 50) == 0:\n","        print(i, loss)\n","    for param_name in ['W1', 'b1', 'W2', 'b2']:\n","        param = getattr(net, param_name)\n","        grad = getattr(net, param_name + '_grad')\n","        param[:] = param - grad"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 0.7784596548632212\n","50 0.4173050986037896\n","100 0.269363878528261\n","150 0.22060437111490086\n","200 0.20256041447587372\n","250 0.1938722206667413\n","300 0.18888207426078868\n","350 0.18573426476143748\n","400 0.18359660558161184\n","450 0.18206156209087793\n"],"name":"stdout"}]}]}